# DCGAN for Anime Face Generation
This project implements a Deep Convolutional Generative Adversarial Network (DCGAN) using TensorFlow and Keras to generate anime faces from random noise. It uses the "anime-faces" dataset to train both the generator and discriminator networks of the GAN.

# Dataset
The dataset used for training is the Anime Faces dataset, which contains a collection of 21,551 images of anime faces. These images are resized to 64x64 pixels and normalized to fit within the range of [-1, 1].

# Model Architecture
## Generator
The generator model takes a latent vector (random noise) as input and generates a 64x64 RGB image. It uses transposed convolutional layers to upsample the latent vector into an image. The network consists of the following layers:

Dense layer to expand the latent vector into a 8x8x512 feature map.
Several transposed convolutional layers to progressively upsample the feature map.
Final output layer with a tanh activation to scale the output to the range [-1, 1].
## Discriminator
The discriminator is a convolutional neural network (CNN) that classifies whether an image is real (from the dataset) or fake (generated by the generator). It uses the following layers:

Several convolutional layers with LeakyReLU activations to extract features from the image.
A final fully connected layer with a sigmoid activation to output the probability of the image being real or fake.
Combined DCGAN Model
The DCGAN model combines the generator and discriminator:

The generator creates images, and the discriminator attempts to classify them as real or fake.
The DCGAN model is trained in an adversarial manner: the generator aims to fool the discriminator, while the discriminator tries to correctly classify images as real or fake.
Training Process
### Optimizer: Adam optimizer is used for both the generator and discriminator with learning rates of 0.0001 for the discriminator and 0.0003 for the generator.
### Loss Function: Binary Crossentropy loss is used to measure the performance of both the generator and discriminator.
### Epochs: The model is trained for 15 epochs, with the training process being monitored visually through generated images.
## Training Results
### Evaluation Metrics:
### Generator Loss: Measures how well the generator is able to produce images that deceive the discriminator.
### Discriminator Loss: Measures how well the discriminator can distinguish between real and fake images.
These losses are tracked during training for each epoch and updated at every training step.

## Generated Images:
At the end of each epoch, a batch of 25 generated images is displayed to provide a visual representation of the generator's progress. These images are created from random noise vectors and demonstrate the quality of generated faces over time.

## Quantitative Metrics (Sample Metrics After Training)
After training for 15 epochs, the following metrics were observed:

Final Generator Loss: 1.9247
Indicates how well the generator is able to produce images that are similar to the real ones.

Final Discriminator Loss: 0.4529
Indicates the performance of the discriminator in distinguishing real and generated images.

Generated Image Quality:

At the end of training, the generated images show a high degree of realism, with faces resembling anime characters.
The model's ability to produce convincing images increases as training progresses, with noticeable improvements in the diversity and accuracy of facial features.
Example Generated Images (Epoch-wise Progress)
Epoch 1: Generated images are rudimentary and lack fine details.
Epoch 5: Faces start to show clearer features like eyes, nose, and mouth.
Epoch 10: Faces are more detailed, with some images closely resembling real anime faces.
Epoch 15: Generated images are high-quality with well-defined facial features, including anime-style eyes, hair, and background textures.
Requirements
To run this project, you'll need to have the following Python packages installed:

tensorflow
keras
matplotlib
numpy
pandas
tqdm
kagglehub
os
